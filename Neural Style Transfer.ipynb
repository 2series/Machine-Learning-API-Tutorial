{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo\n",
    "\n",
    "We're going to build a Neural Style Transfer model, train it in the cloud, then create an API that we can use to send images to and have an artistic filter applied to them!\n",
    "\n",
    "- Step 1 - Building our Model\n",
    "- Step 2 - Training the model on FloydHub\n",
    "- Step 3 - Serving the model via an API\n",
    "\n",
    "![alt text](https://blog.paperspace.com/content/images/2017/02/PaperspaceTransferLearningTutorial01.png \"Logo Title Text 1\")\n",
    "\n",
    "#### Building the Model\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*btAtU_VrgmKBbG1gakXV2w.png \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/sunshineatnoon/Paper-Collection/master/images/RTNS.png \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "- There are 3 parts to the workflow! A content extractor, A style extractor , and a merger. \n",
    "\n",
    "![alt text](https://blog.paperspace.com/content/images/2017/02/Untitled-Diagram-4-.png \"Logo Title Text 1\")\n",
    "\n",
    "###### Part 1 - The Content Extractor\n",
    "\n",
    "![alt text](https://blog.paperspace.com/content/images/2017/02/Untitled-Diagram-10-.png \"Logo Title Text 1\")\n",
    "\n",
    "- They seperated the semantic content of an image!\n",
    "- They used a convolutional neural network called VGG 19. \n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*f3wRS2crHnQ7Pu0x6FYuIQ.gif \"Logo Title Text 1\")\n",
    "\n",
    "- ConvNets are neural networks that are well-suited for image classification tasks\n",
    "- VGG 19 was trained on thousands of images and is capable of classifying images out of the box. \n",
    "- It looks like they used the output of one of the hidden layers as a content extractor. \n",
    "- Thats because the hidden layers of a ConvNet extract high level features of an image, and the deeper the layer, the more high level the attributes will be that the layer identifies. \n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*eg2MRfNFxIqjG_UK3vo0DQ.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*GksqN5XY8HPpIddm5wzm7A.jpeg \"Logo Title Text 1\")\n",
    "\n",
    "- Between taking an image as input and outputting a guess as to what it is, a CNN is doing transformations to turn the image pixels into an internal understanding of the content of the image. \n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*EvBcni8o_O3v4RUl640TZQ@2x.png \"Logo Title Text 1\")\n",
    "\n",
    "- We can use one of the intermediate semantic representations in a ConvNet to compare the content of two images. \n",
    "- If we pass 2 different images through a ConvNet after being passed through a few hidden layers, their representations will be very close in raw value. \n",
    "- If we pass both the final image and the content image and find the distance between the intermediate representation of those images, we have the content loss. \n",
    "- We make a list of layers at which we want to compute content loss. \n",
    "- We pass both images through the network until a particular layer in the list, take it out of that layer, square the difference between each corresponding value in the output and sum them all up.\n",
    "- We do this for every layer in the list and sum those up. \n",
    "- We’re also multiplying each of the representations by some value alpha, called content weight after finding their differences and squaring it.\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*Sbis79TMJ7f7qIetlEAqqA.png \"Logo Title Text 1\")\n",
    "\n",
    "###### Part 2 - The Style Extractor\n",
    "\n",
    "![alt text](https://blog.paperspace.com/content/images/2017/02/Untitled-Diagram-11-.png \"Logo Title Text 1\")\n",
    "\n",
    "- Same idea as the content extractor, meaning they used the output of a hidden layer but they added an additional step. \n",
    "- It used a correlation estimator based on the gram matrix of the filters of a given hidden layer.\n",
    "- This destroys semantics of the image but preserves its basic components, making an excellent texture extractor. \n",
    "- A gram matrix results from multiple a matrix with the transpose of itself. \n",
    "- And because every column is multiplied with every row in the matrix, we can think of the spatial information that was contained in the original representations to have been distributed. \n",
    "- This game matrix contains all sorts of information about the image, the texture, shapes, and style. \n",
    "- Once we have the gram matrix, we can find the distance between the gram matrices of the intermediate representations of both our image and the style image to find how similar they are in style. \n",
    "- And its all multiple by some value beta, known as the style weight. \n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*ZJjUFPPqLZ1z48maIfInBA.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*R3Ler_uVVldfdRSYmeLKjw.png \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "###### Part 3 - Blending Content + Style\n",
    "\n",
    "![alt text](https://blog.paperspace.com/content/images/2017/02/Untitled-Diagram-1-.jpg \"Logo Title Text 1\")\n",
    "\n",
    "- They of course framed it as an optimization problem as machine learning papers tend to do. \n",
    "- And in an optimization problem, some cost function is minimized iteratively during training to achieve a goal. \n",
    "- Their cost function penalized the synthesized image if its content was not equal to the desired content and its style was not equal to the desired style.  \n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*3-60SfuOkU0LMoAspntCSA.png \"Logo Title Text 1\")\n",
    "\n",
    "- Both the content and style loss were added together to get the cost function. \n",
    "- They then performed back propagation to minimize the cost by getting the gradient of the final image and iteratively changing it to look more and more like a stylized content image. \n",
    "- They used an optimization technique thats terribly named L-BFGS which isn’t as popular as say stochastic gradient descent. \n",
    "- If do a bit of research it looks like its a second order optimization scheme, meaning it uses the derivative of the derivative, that gets closer to the global minimum but the iteration cost is also bigger.\n",
    "\n",
    "![alt text](https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs40537-017-0084-5/MediaObjects/40537_2017_84_Figa_HTML.gif \"Logo Title Text 1\")\n",
    "\n",
    "#### FloydHub\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*fD0KwPktgmf1zn8pSrrtUg.jpeg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1203/0*DgHKkUPnT9Qa9K8L. \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
